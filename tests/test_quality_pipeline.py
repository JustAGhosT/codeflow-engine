"""
Comprehensive tests for the Quality Pipeline

Tests all quality modes, edge cases, and integration scenarios.
"""

import tempfile
from pathlib import Path

import pytest

from codeflow_engine.actions.quality_engine.config import load_config
from codeflow_engine.actions.quality_engine.engine import QualityEngine, QualityInputs
from codeflow_engine.actions.quality_engine.models import QualityMode


class TestQualityPipeline:
    """Test suite for the Quality Pipeline."""

    @pytest.fixture
    def temp_dir(self):
        """Create a temporary directory for test files."""
        with tempfile.TemporaryDirectory() as temp_dir:
            yield temp_dir

    @pytest.fixture
    def quality_engine(self):
        """Create a quality engine instance."""
        return QualityEngine()

    @pytest.fixture
    def sample_python_file(self, temp_dir):
        """Create a sample Python file with various issues."""
        file_path = Path(temp_dir) / "test_file.py"
        content = '''
import os
import sys

def unused_function():
    """This function is unused."""
    pass

def function_with_issues():
    x = 1
    y = 2
    print("Hello world")  # Missing f-string
    return x + y

class TestClass:
    def __init__(self):
        self.value = None

    def method_with_issues(self):
        """Method with various issues."""
        unused_var = "unused"
        result = 1 + 2
        return result

# Missing type hints
def no_type_hints(param):
    return param * 2

# Security issue
password = "hardcoded_password_123"

# Complexity issue
def complex_function():
    x = 1
    if x > 0:
        if x < 10:
            if x == 5:
                if x != 3:
                    if x >= 1:
                        return x
    return 0
'''
        file_path.write_text(content)
        return str(file_path)

    @pytest.fixture
    def sample_js_file(self, temp_dir):
        """Create a sample JavaScript file."""
        file_path = Path(temp_dir) / "test_file.js"
        content = """
// JavaScript file with some issues
function unusedFunction() {
    console.log("This function is unused");
}

function functionWithIssues() {
    var x = 1;
    var y = 2;
    console.log("Hello world");
    return x + y;
}

// Missing semicolons
var test = "test"
var another = "another"

// Unused variable
var unused = "unused";
"""
        file_path.write_text(content)
        return str(file_path)

    @pytest.fixture
    def empty_file(self, temp_dir):
        """Create an empty file."""
        file_path = Path(temp_dir) / "empty.py"
        file_path.write_text("")
        return str(file_path)

    @pytest.fixture
    def large_file(self, temp_dir):
        """Create a large file for testing."""
        file_path = Path(temp_dir) / "large_file.py"
        content = []
        for i in range(1000):
            content.append(f"def function_{i}():")
            content.append(f"    return {i}")
            content.append("")
        file_path.write_text("\n".join(content))
        return str(file_path)

    def test_quality_engine_initialization(self, quality_engine):
        """Test that the quality engine initializes correctly."""
        assert quality_engine is not None
        assert hasattr(quality_engine, "execute")
        assert hasattr(quality_engine, "tool_registry")

    def test_config_loading(self):
        """Test configuration loading."""
        config = load_config()
        assert config is not None
        assert hasattr(config, "default_mode")
        assert hasattr(config, "tools")

    @pytest.mark.asyncio
    async def test_fast_mode(self, quality_engine, sample_python_file):
        """Test Fast mode with basic checks."""
        inputs = QualityInputs(
            mode=QualityMode.FAST, files=[sample_python_file], enable_ai_agents=False
        )

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        assert hasattr(result, "success")
        assert hasattr(result, "total_issues_found")
        assert hasattr(result, "issues_by_tool")

        # Fast mode should execute successfully (tools may fail on Windows)
        assert hasattr(result, "tool_execution_times")
        assert isinstance(result.tool_execution_times, dict)

    @pytest.mark.asyncio
    async def test_comprehensive_mode(self, quality_engine, sample_python_file):
        """Test Comprehensive mode with all tools."""
        inputs = QualityInputs(
            mode=QualityMode.COMPREHENSIVE,
            files=[sample_python_file],
            enable_ai_agents=False,
        )

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        assert hasattr(result, "success")
        assert hasattr(result, "total_issues_found")

        # Comprehensive mode should execute successfully (tools may fail on Windows)
        assert hasattr(result, "tool_execution_times")
        assert isinstance(result.tool_execution_times, dict)

    @pytest.mark.asyncio
    async def test_smart_mode(self, quality_engine, sample_python_file):
        """Test Smart mode with context-aware tool selection."""
        inputs = QualityInputs(
            mode=QualityMode.SMART,
            files=[sample_python_file],
            volume=500,  # Medium volume
            enable_ai_agents=False,
        )

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        assert hasattr(result, "success")
        assert hasattr(result, "total_issues_found")

    @pytest.mark.asyncio
    async def test_ai_enhanced_mode(self, quality_engine, sample_python_file):
        """Test AI-Enhanced mode (mocked LLM)."""
        inputs = QualityInputs(
            mode=QualityMode.AI_ENHANCED,
            files=[sample_python_file],
            enable_ai_agents=True,
            ai_provider="openai",
            ai_model="gpt-4",
        )

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        assert hasattr(result, "success")
        assert hasattr(result, "ai_enhanced")
        # AI mode should be marked as enhanced even if LLM fails
        assert result.ai_enhanced in [True, False]

    @pytest.mark.asyncio
    async def test_ultra_fast_mode(self, quality_engine, sample_python_file):
        """Test Ultra-Fast mode with minimal checks."""
        inputs = QualityInputs(
            mode=QualityMode.ULTRA_FAST,
            files=[sample_python_file],
            enable_ai_agents=False,
        )

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        assert hasattr(result, "success")
        assert hasattr(result, "total_issues_found")

    @pytest.mark.asyncio
    async def test_empty_file_list(self, quality_engine):
        """Test behavior with empty file list."""
        inputs = QualityInputs(mode=QualityMode.FAST, files=[], enable_ai_agents=False)

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        assert result.total_issues_found == 0
        assert result.success is True

    @pytest.mark.asyncio
    async def test_nonexistent_files(self, quality_engine):
        """Test behavior with nonexistent files."""
        inputs = QualityInputs(
            mode=QualityMode.FAST, files=["nonexistent_file.py"], enable_ai_agents=False
        )

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        # Should handle gracefully without crashing

    @pytest.mark.asyncio
    async def test_mixed_file_types(
        self, quality_engine, sample_python_file, sample_js_file
    ):
        """Test with mixed file types."""
        inputs = QualityInputs(
            mode=QualityMode.SMART,
            files=[sample_python_file, sample_js_file],
            enable_ai_agents=False,
        )

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        assert hasattr(result, "success")
        assert hasattr(result, "total_issues_found")

    @pytest.mark.asyncio
    async def test_large_file_handling(self, quality_engine, large_file):
        """Test handling of large files."""
        inputs = QualityInputs(
            mode=QualityMode.FAST, files=[large_file], enable_ai_agents=False
        )

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        assert hasattr(result, "success")

    @pytest.mark.asyncio
    async def test_volume_based_configuration(self, quality_engine, sample_python_file):
        """Test volume-based configuration."""
        # Test different volume levels
        volumes = [100, 300, 500, 700, 900]

        for volume in volumes:
            inputs = QualityInputs(
                mode=QualityMode.SMART,
                files=[sample_python_file],
                volume=volume,
                enable_ai_agents=False,
            )

            result = await quality_engine.execute(inputs, {})

            assert result is not None
            assert hasattr(result, "success")

    @pytest.mark.asyncio
    async def test_auto_fix_dry_run(self, quality_engine, sample_python_file):
        """Test auto-fix in dry-run mode."""
        inputs = QualityInputs(
            mode=QualityMode.FAST,
            files=[sample_python_file],
            auto_fix=True,
            dry_run=True,
            enable_ai_agents=False,
        )

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        assert hasattr(result, "auto_fix_applied")
        assert hasattr(result, "fix_summary")

    def test_quality_mode_from_volume(self):
        """Test volume to quality mode mapping."""
        from codeflow_engine.enums import QualityMode

        # Test volume thresholds
        assert QualityMode.from_volume(0) == QualityMode.ULTRA_FAST
        assert QualityMode.from_volume(100) == QualityMode.FAST
        assert QualityMode.from_volume(300) == QualityMode.SMART
        assert QualityMode.from_volume(600) == QualityMode.COMPREHENSIVE
        assert QualityMode.from_volume(800) == QualityMode.AI_ENHANCED

    def test_quality_mode_validation(self):
        """Test quality mode validation."""
        from codeflow_engine.enums import QualityMode

        # Test valid modes
        assert QualityMode("fast") == QualityMode.FAST
        assert QualityMode("comprehensive") == QualityMode.COMPREHENSIVE
        assert QualityMode("ai_enhanced") == QualityMode.AI_ENHANCED
        assert QualityMode("smart") == QualityMode.SMART

    @pytest.mark.asyncio
    async def test_tool_execution_times(self, quality_engine, sample_python_file):
        """Test that tool execution times are tracked."""
        inputs = QualityInputs(
            mode=QualityMode.FAST, files=[sample_python_file], enable_ai_agents=False
        )

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        assert hasattr(result, "tool_execution_times")
        assert isinstance(result.tool_execution_times, dict)

    @pytest.mark.asyncio
    async def test_issues_by_tool(self, quality_engine, sample_python_file):
        """Test that issues are properly categorized by tool."""
        inputs = QualityInputs(
            mode=QualityMode.FAST, files=[sample_python_file], enable_ai_agents=False
        )

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        assert hasattr(result, "issues_by_tool")
        assert isinstance(result.issues_by_tool, dict)

    @pytest.mark.asyncio
    async def test_files_by_tool(self, quality_engine, sample_python_file):
        """Test that files with issues are properly categorized by tool."""
        inputs = QualityInputs(
            mode=QualityMode.FAST, files=[sample_python_file], enable_ai_agents=False
        )

        result = await quality_engine.execute(inputs, {})

        assert result is not None
        assert hasattr(result, "files_by_tool")
        assert isinstance(result.files_by_tool, dict)

    def test_quality_inputs_validation(self):
        """Test QualityInputs validation."""
        # Test valid inputs
        inputs = QualityInputs(
            mode=QualityMode.FAST,
            files=["test.py"],
            max_fixes=10,
            max_issues=50,
            enable_ai_agents=False,
        )

        assert inputs.mode == QualityMode.FAST
        assert inputs.files == ["test.py"]
        assert inputs.max_fixes == 10
        assert inputs.max_issues == 50
        assert inputs.enable_ai_agents is False

    def test_quality_inputs_volume_application(self):
        """Test volume-based settings application."""
        inputs = QualityInputs(volume=500)
        inputs.apply_volume_settings()

        # Volume 500 should map to SMART mode
        assert inputs.mode == QualityMode.SMART
        assert inputs.enable_ai_agents is True

    @pytest.mark.asyncio
    async def test_error_handling(self, quality_engine):
        """Test error handling with invalid inputs."""
        # Test with invalid file
        inputs = QualityInputs(
            mode=QualityMode.FAST,
            files=["/invalid/path/file.py"],
            enable_ai_agents=False,
        )

        result = await quality_engine.execute(inputs, {})

        # Should not crash and should return a result
        assert result is not None
        assert hasattr(result, "success")

    @pytest.mark.asyncio
    async def test_performance_benchmark(self, quality_engine, sample_python_file):
        """Test performance of different modes."""
        import time

        modes = [QualityMode.ULTRA_FAST, QualityMode.FAST, QualityMode.SMART]
        results = {}

        for mode in modes:
            inputs = QualityInputs(
                mode=mode, files=[sample_python_file], enable_ai_agents=False
            )

            start_time = time.time()
            result = await quality_engine.execute(inputs, {})
            end_time = time.time()

            results[mode] = {
                "execution_time": end_time - start_time,
                "issues_found": result.total_issues_found,
                "success": result.success,
            }

        # Verify that faster modes execute more quickly (with tolerance for system load)
        tolerance = 0.1  # 100ms tolerance for system load variations
        
        ultra_fast_time = results[QualityMode.ULTRA_FAST]["execution_time"]
        fast_time = results[QualityMode.FAST]["execution_time"]
        smart_time = results[QualityMode.SMART]["execution_time"]
        
        assert (
            ultra_fast_time <= fast_time + tolerance
        ), f"Ultra-fast mode ({ultra_fast_time:.3f}s) should be faster than fast mode ({fast_time:.3f}s)"
        
        assert (
            fast_time <= smart_time + tolerance
        ), f"Fast mode ({fast_time:.3f}s) should be faster than smart mode ({smart_time:.3f}s)"


if __name__ == "__main__":
    pytest.main([__file__])
