"""
Data models for the AutoPR Agent Framework.

This module defines the Pydantic models used for type-safe data exchange between agents.
"""

from enum import Enum
from typing import Any

from pydantic import BaseModel, Field


class IssueSeverity(str, Enum):
    """Severity levels for code issues."""

    WARNING = "warning"
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class CodeIssue(BaseModel):
    """Represents a single code quality issue."""

    file_path: str = Field(..., description="Path to the file containing the issue")
    line_number: int = Field(..., description="Line number where the issue occurs")
    column: int = Field(0, description="Column number where the issue occurs")
    message: str = Field(..., description="Description of the issue")
    severity: IssueSeverity = Field(..., description="Severity level of the issue")
    rule_id: str = Field(..., description="Identifier for the rule that was violated")
    category: str = Field(
        "style", description="Category of the issue (e.g., 'performance', 'security')"
    )
    fix: dict[str, Any] | None = Field(
        None, description="Suggested fix for the issue, if available"
    )

    # Backward-compat aliases used by some tests
    @classmethod
    def model_construct(cls, *_args, **_kwargs):  # type: ignore[override]
        return super().model_construct(*_args, **_kwargs)

    def __init__(self, **data):
        # Accept alias keys from tests
        if "line" in data and "line_number" not in data:
            data["line_number"] = data.pop("line")
        if "column_number" in data and "column" not in data:
            data["column"] = data.pop("column_number")
        if "severity" in data and isinstance(data["severity"], str):
            from contextlib import suppress

            with suppress(Exception):
                data["severity"] = IssueSeverity(data["severity"])  # type: ignore[arg-type]
        # Ignore unknown fix fields used in tests to create issues
        for extra_key in ["fix_suggestion", "fix_confidence"]:
            data.pop(extra_key, None)
        super().__init__(**data)


class PlatformComponent(BaseModel):
    """Represents a component of the technology stack."""

    name: str = Field(..., description="Name of the component")
    version: str | None = Field(
        None, description="Version of the component, if detected"
    )
    confidence: float = Field(
        ..., ge=0, le=1, description="Confidence score for this detection (0-1)"
    )
    evidence: list[str] = Field(
        default_factory=list, description="List of evidence supporting this detection"
    )


class PlatformAnalysis(BaseModel):
    """Analysis of the platform and technology stack."""

    platform: str = Field(..., description="Primary platform/framework detected")
    confidence: float = Field(
        ..., ge=0, le=1, description="Confidence score for platform detection (0-1)"
    )
    components: list[PlatformComponent] = Field(
        default_factory=list, description="List of detected platform components"
    )
    recommendations: list[str] = Field(
        default_factory=list, description="List of recommendations for the platform"
    )


class CodeAnalysisReport(BaseModel):
    """Comprehensive code analysis report."""

    issues: list[CodeIssue] = Field(
        default_factory=list, description="List of code quality issues found"
    )
    metrics: dict[str, float] = Field(
        default_factory=dict, description="Code quality metrics"
    )
    summary: str = Field(..., description="Human-readable summary of the analysis")
    platform_analysis: PlatformAnalysis = Field(
        ..., description="Analysis of the platform and technology stack"
    )

    def to_dict(
        self,
        *,
        include: set[str] | None = None,
        exclude: set[str] | None = None,
        by_alias: bool = False,
        exclude_unset: bool = False,
        exclude_defaults: bool = False,
        exclude_none: bool = False,
    ) -> dict[str, Any]:
        """Convert the report to a dictionary using Pydantic v2 serialization.

        Args:
            include: Fields to include in the output
            exclude: Fields to exclude from the output
            by_alias: Whether to use field aliases in the output
            exclude_unset: Whether to exclude fields that were not explicitly set
            exclude_defaults: Whether to exclude fields that are set to their default value
            exclude_none: Whether to exclude fields that are None

        Returns:
            Dictionary representation of the model
        """
        return self.model_dump(
            include=include,
            exclude=exclude,
            by_alias=by_alias,
            exclude_unset=exclude_unset,
            exclude_defaults=exclude_defaults,
            exclude_none=exclude_none,
        )

    def to_json(self, *, indent: int | None = None, **kwargs) -> str:
        """Convert the report to a JSON string using Pydantic v2 serialization.

        Args:
            indent: Number of spaces for JSON indentation
            **kwargs: Additional arguments passed to model_dump_json()

        Returns:
            JSON string representation of the model
        """
        return self.model_dump_json(indent=indent, **kwargs)

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "CodeAnalysisReport":
        """Create a report from a dictionary using Pydantic v2 validation.

        Args:
            data: Dictionary containing the report data

        Returns:
            A new CodeAnalysisReport instance
        """
        return cls.model_validate(data)

    @classmethod
    def from_json(cls, json_data: str | bytes | bytearray) -> "CodeAnalysisReport":
        """Create a report from a JSON string using Pydantic v2 validation.

        Args:
            json_data: JSON string or bytes containing the report data

        Returns:
            A new CodeAnalysisReport instance
        """
        return cls.model_validate_json(json_data)
